{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 统计分析-回归分析与分类分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. 概述与大纲"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 从建模的目的看回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回归分析与分类分析都是一种基于统计模型的统计分析方法。它们都研究因变量（被解释变量）与自变量（解释变量）之间存在的潜在关系，并通过统计模型的形式将这些潜在关系进行显式的表达。不同的是，回归分析中因变量是连续变量，如工资、销售额；而分类分析中因变量是属性变量，如判断邮件“是or否”为垃圾邮件。\n",
    "\n",
    "上一段我们提到，回归是一种基于统计模型的分析方法，因此回归分析的过程本质上一种建模过程。统计建模的主要任务有二：预测与推断。\n",
    "\n",
    "所谓预测，就是利用一个训练完毕的模型$\\hat{f}$，根据输入的自变量$X$获得对应的输出$Y$。在预测任务中，如果模型$\\hat{f}$可以准确地提供预测，那么$\\hat{f}$是什么形式并不重要，而如果$\\hat{f}$的形式非常复杂且难以解释，我们可以将之称为黑盒模型(Black Box)。举一个例子，假设$X_1,X_2,\\cdots ,X_p$是某个病人的血样特征，$Y$测量了病人使用药物后出现严重不良反应的风险，那么如果存在一个模型可以很好地通过$X$以预测$Y$，那自然是再好不过的事了。此时，模型的形式、变量之间的关系在正确预测面前都显得不那么重要。事实上，当前具有强大预测性能的模型大多都是黑盒模型，如强大的Xgboost机器学习算法以及各种深度学习算法，它们的模型可解释性差，我们难以解释其中一些参数的含义与统计性质。\n",
    "\n",
    "与预测相对应的另一任务便是推断。在很多情况下，我们对当$X_1,X_2,\\cdots ,X_p$变化时**如何影响**$Y$更感兴趣，此时，我们估计模型$\\hat{f}$的目的不是为了预测$Y$，而是想明白两者之间的关系，更深层次地讲，我们想要知道模型内各种参数的数值与统计推断性质等等。在这种情况下，模型的可解释性就非常重要了，而通常我们在推断任务中最常使用的模型正是线性回归模型。举一个例子，在研究各因素对商品销售量的场景中，我们会更关注以下问题：哪类媒体对销量有直接的贡献？增加电视广告费用能对销售量带来多少程度的增加？等等，这就是典型的推断问题。\n",
    "\n",
    "弄清楚了预测与推断的区别，我们重新审视一下回归分析：回归分析更加注重对因变量与自变量之间潜在关系的推断，所使用的统计模型也相对简单（一般为线性模型），如果你在比赛中需要分析各变量间的潜在相关关系，便可以考虑使用回归分析。\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 课程大纲"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本轮课程，我们将先后学习回归分析与分类分析的知识。其中，回归分析中我们主要学习经典线性模型(CLM, Classical Linear Model)与最小二乘估计(OLS)，广义线性模型(GLM)只做简单介绍；分类分析中我们主要学习基于线性模型的Logistics模型与Probit模型。具体大纲如下：\n",
    "\n",
    "· 回归的思想与线性回归模型介绍\n",
    "\n",
    "· OLS估计在经典线性回归模型假设下的统计推断\n",
    "\n",
    "· 线性回归模型中的参数检验\n",
    "\n",
    "· 线性回归模型设定的误差分析\n",
    "\n",
    "· 异方差下回归建模的解决方法\n",
    "\n",
    "首先，我们将了解回归的基本思想，并对最常用的回归模型-经典线性模型的模型形式、模型假设（也称为CLM假设）做基本的介绍。\n",
    "\n",
    "知晓了模型的形式后，下一步自然是进行模型参数的估计，并推断估计参数的统计性质。在线性模型中，这些参数就是每个自变量的系数，我们想知道：使用何种方法进行参数估计呢？参数在这种估计方法下能否接近真实参数呢？估计的误差有多大呢？我们将学习使用最小二乘法(OLS)对线性模型进行估计，并探究OLS估计下各参数的统计性质。可以告诉大家，在满足CLM假设的前提下，OLS估计是经典线性模型最优的参数估计法；基于CLM假设与OLS估计，我们便可以对模型进行各种假设检验，包括参数显著性检验，模型显著性检验等等。\n",
    "\n",
    "然而，理想很丰满，现实很骨感，我们所获得的实验数据不总是能满足CLM假设中的每一条假设。某个假设不成立会给模型参数估计的准确性（无偏性）、稳定性（方差）以及假设检验带来多少影响呢？这就是模型设定误差分析需要研究的内容。最后，若数据不满足CLM假设中的某个假设，需要找到对应的解决办法，在本轮课程，我们重点探讨不满足同方差假设下参数估计方法与参数检验方法的改进。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 回归模型总述"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这一章节，我们将对“回归”进行宏观的介绍，使大家对回归有直观的理解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 回归思想与一般回归模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 横截面数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "横截面数据是回归分析最主要的分析数据类型，它可以视为在**同一时间点（或抽样时间差异可以被忽略）**上对**多个抽样个体**的观测数据。通常，我们记第$i$个个体的观测数据为$(x_i,y_i)$。如果以抽样时间点与抽样个体数目为维度划分数据类型，除了横截面数据外，还有时间序列数据以及面板数据。时间序列数据为单个个体在不同时间点上的观测数据，而面板数据则是多个个体在不同时间点上的观测数据。对时间序列数据的分析需要用到时间序列分析的知识，对面板数据的分析则是高级计量经济学的内容，在本次课程我们不对它们做介绍。三者的区别如下图所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/横截面数据.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 回归思想——条件均值建模"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "横截面数据最重要的一个特征，就是我们可以将采集的数据$(x_1,y_1),(x_2,y_2),…,(x_n,y_n)$近似视为来自一个潜在总体的随机样本，即假设\n",
    "$$\n",
    "\\left(x_{1}, y_{1}\\right), \\cdots,\\left(x_{n}, y_{n}\\right) \\sim^{i i d}(x, y)\n",
    "$$\n",
    "我们进行数据分析的最终目的是为了找到$x$与$y$之间的关系并用模型显性表示出来，此时最理想的状态是使用一个**条件分布**刻画$x$对$y$的影响\n",
    "$$\n",
    "F_{y \\mid x}\n",
    "$$\n",
    "即在任意给定$x$的条件下都有一个明确的分布$F$刻画$y$的状态。但是在实际问题中，直接估计这个条件分布几乎是一件不可能的事，且我们也难以对分布进行解释与应用。于是，我们退而求其次通过分布的一般数字特征对两者的关系进行推断，如条件分布的中心位置，形状，即考虑**条件均值、条件方差**\n",
    "$$\n",
    "E(y \\mid x), \\operatorname{Var}(y \\mid x)\n",
    "$$\n",
    "而回归正是利用条件均值$E(y \\mid x)$来刻画$x$与$y$的关系，回归建模的本质也正是“条件均值的建模”。那么，怎么理解条件均值建模呢？我们举一个不典型的例子帮助大家理解。\n",
    "\n",
    "假设某个样本量为100的数据集中，自变量$x$有1,2,3,4,5五个值，样本的因变量$y$都来自以其自变量为均值，方差为1的正态分布。我们想要刻画因变量与自变量之间的变化关系，就要找出可以代表各种类样本内（在此例中以自变量为划分依据）共性的特征，用这些特征来描绘变化关系。最直观也是最简单的特征就是条件均值，即给定$x$的条件下样本的均值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'E(Y|X)')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x=list()\n",
    "y=list()\n",
    "for i in [1,2,3,4,5]:\n",
    "    y_norm=stats.norm.rvs(i,1,20,random_state=i).tolist()\n",
    "    y.extend(y_norm)\n",
    "    x1=np.ones(20)*i\n",
    "    x1=x1.tolist()\n",
    "    x.extend(x1)\n",
    "\n",
    "data={'x':x,'y':y}\n",
    "df=pd.DataFrame(data)\n",
    "\n",
    "sns.regplot(x='x',y='y',data=df)\n",
    "plt.title('E(Y|X)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据上图我们可以看出，$E(y|x=1)=1$,$E(y|x=2)=2$,…,$E(y|x=x_0)=x_0$。通过条件均值，我们可以推断出$x$与$y$的关系可以用模型$y=x+u$来刻画，其中，$u$被称为随机误差，可理解为：除$x$外，其他影响$y$取值的因素。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 一般回归模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上述例子中，我们使用模型$y=x+u$刻画了$x$与$y$的关系，这说明了在这个数据集中我们将模型设定为了\n",
    "$$\n",
    "y=x+u\n",
    "$$\n",
    "事实上，如果我们将上述公式中的$x$泛化成条件均值$E(y|x)$，那么我们就能得到最一般的回归模型\n",
    "$$\n",
    "y=E(y|x)+u\n",
    "$$\n",
    "这也就意味着，所谓回归模型的建模，**本质上就是条件均值建模**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**· 回归模型的条件解读**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般回归模型$y=E(y|x)+u$暗含了一个天然成立的假设：\n",
    "1. 随机误差的条件期望$E(u|x)=0$\n",
    "\n",
    "利用重期望公式，我们可以根据假设1进一步推得下面两个推论:\n",
    "\n",
    "推论1. 随机误差的无条件期望$E(u)=0$——这表示其他因素对$y$的平均影响为0\n",
    "<br>\n",
    "推论2. 随机误差$u$与自变量$x$协方差$Cov(u,x)=0$——这表示其他因素与参与回归的$x$不相关！\n",
    "\n",
    "根据假设1，我们可以将一般回归模型表示成一种新的形式：\n",
    "$$\n",
    "y=E(y|x)+u\\Longleftrightarrow y=m\\left( x \\right) +u, where\\,\\,E\\left( u|x \\right) =0\n",
    "$$\n",
    "在这里，$E(u|x)=0$等价于$m(x)=E(y|x)$。事实上，用这一种形式表示回归模型更常见，也更有利于接下来对模型$m(x)$具体形式的假定，因为这告诉了我们：只要假定随机误差$u$与$x$不相关（这里可理解为其他影响$y$的外生因素与内生因素$x$不相关），我们就可以根据需要假定回归模型的具体形式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 线性回归模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 线性模型形式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的学习中我们介绍了回归模型的一般形式。在实际建模中，为了有效的估计，我们必须对模型中$m(x)$的形式进行具体的假定。在所有模型假定形式中，线性回归模型是最常用假定形式，也是回归分析中最重要的模型，是本次课程重点讲解的内容。\n",
    "\n",
    "线性模型假设有：\n",
    "$$\n",
    "m(x)=\\beta_{0}+\\beta_{1} x_{1}+\\cdots+\\beta_{p} x_{p}\n",
    "$$\n",
    "于是，线性回归模型可表示为：\n",
    "$$\n",
    "y=\\beta_{0}+\\beta_{1} x_{1}+\\cdots+\\beta_{p} x_{p}+u, \\quad E\\left(u \\mid x_{1}, \\cdots, x_{p}\\right)=0\n",
    "$$\n",
    "回归分析主要研究如何有效地估计模型中的参数$\\hat{\\beta}_i$，并利用模型进行推断与预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 从简单线性回归到多元线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**· 用简单线性回归理解对模型的解释**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为大家快速理解线性回归模型，我们先假设$x$是一维的，即只考虑一个因素对$y$的影响，此时亦称模型为简单线性回归，形式为\n",
    "$$\n",
    "y=\\beta_{0}+\\beta_{1} x+u, \\quad E(u \\mid x)=0\n",
    "$$\n",
    "$\\beta_{0}$是截距项，可以理解为$x=0$时$y$的期望值，一般情况下，如果我们回归的任务是推断，则截距通常不重要；\n",
    "<br>\n",
    "$\\beta_{1}=\\frac{\\Delta m(x)}{\\Delta x}$，可理解为$x$每增加一个单位，$y$**平均**增加$\\beta_1$个单位。\n",
    "\n",
    "此后，我们将默认模型含有$E(u|x)=0$的设定（因为只有这样模型才代表回归模型），该条件不再以书面形式写出。\n",
    "\n",
    "我们举一个例子帮助大家理解：\n",
    "\n",
    "**Example1.** 假设大学成绩colGPA与大学测验水平ACT间关系为\n",
    "$$\n",
    "\\text { colGPA }=\\beta_{0}+\\beta_{1} \\text { hsGPA }+u\n",
    "$$\n",
    "$\\beta_1$系数的解释为：每增加1单位大学测验水平，大学成绩会增加$\\beta_1$个单位；由于该模型中自变量只有高中成绩，而大学成绩水平肯定还受其他因素影响，因此该模型中的随机误差包含了如高中成绩、自主学习能力等因素。\n",
    "\n",
    "注意：设定$E(u|x)=0$的存在暗含了**在该模型中**高中测验成绩、自主学习能力等因素与自变量大学测验水平无关，但这在**实际问题中**未必成立。而一旦它们存在相关性，就意味着模型假设不符合实际情况，模型估计的有效性与准确性也将受到影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**· 进行全面的回归建模——多元线性回归**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单纯的简单线性模型有很大的局限性，原因有二：\n",
    "\n",
    "1、在实际问题中，因变量$y$通常受多个因素影响，这些因素之间可能彼此之间存在线性相关性（后续的学习中我们将这种现象称为多重共线性），而默认假设$E(u|x)=0$的直接推论(推论2)就是其他影响因素与$x$线性无关，显然不一定符合实际情况。\n",
    "\n",
    "2、如果我们想推断一个变量对另一个变量的因果关系，就要保持尽可能多的其他因素的不变，因此需要尽量把关键因素纳入到回归模型当中，这样便可以控制多个变量，查看某个特定变量变化对自变量的影响。\n",
    "\n",
    "因此在实际问题中，我们更多地使用多元线性回归。一般的多元线性回归模型可写成：\n",
    "$$\n",
    "y=\\beta_{0}+\\beta_{1} x_{1}+\\cdots+\\beta_{k} x_{k}+u\n",
    "$$\n",
    "$u$依旧为随机误差项，它表示除$x_1$,…,$x_k$以外的其他因素对因变量$y$的影响，且同样满足假设\n",
    "$$\n",
    "E\\left(u \\mid x_{1}, \\cdots, x_{k}\\right)=0\n",
    "$$\n",
    "$\\beta_i=\\frac{\\partial m\\left( x \\right)}{\\partial x_i}$是回归函数对变量$x_i$的偏导数，它被解释为**在保持其他自变量不变的情况下，$x_i$每增加一单位，$y$平均增加$\\beta_i$个单位**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**· Example2.** 依旧以上面的大学成绩例子为例，这一次我们增加一个高中成绩hsGPA变量，此时模型变为\n",
    "$$\n",
    "\\mathrm{colGPA}=\\beta _0+\\beta _1\\mathrm{hsGPA}+\\beta _2\\mathrm{ACT}+u\n",
    "$$\n",
    "在模型增加了一个我们认为非常重要的变量后，模型的估计会产生怎样的变化呢？我们使用python对该例的数据集进行回归分析，比较两种模型的区别。具体的python实现过程我们将稍后介绍，大家只需要关注这里的结果即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda\\lib\\site-packages\\statsmodels\\tsa\\tsatools.py:142: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n",
      "  x = pd.concat(x[::order], 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>const</th>\n",
       "      <th>ACT</th>\n",
       "      <th>hsGPA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>3.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>1.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>1.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>3.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>141 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     const   ACT  hsGPA\n",
       "0      1.0  21.0    3.0\n",
       "1      1.0  24.0    3.2\n",
       "2      1.0  26.0    3.6\n",
       "3      1.0  27.0    3.5\n",
       "4      1.0  28.0    3.9\n",
       "..     ...   ...    ...\n",
       "136    1.0  23.0    3.3\n",
       "137    1.0  25.0    3.6\n",
       "138    1.0  21.0    3.4\n",
       "139    1.0  26.0    3.7\n",
       "140    1.0  28.0    3.3\n",
       "\n",
       "[141 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 加载数据\n",
    "gpa1=pd.read_stata('./data/gpa1.dta')\n",
    "\n",
    "# 在数据集中提取自变量\n",
    "X1=gpa1.ACT\n",
    "X2=gpa1[['ACT','hsGPA']]\n",
    "# 提取因变量\n",
    "y=gpa1.colGPA\n",
    "\n",
    "# 为自变量增添截距项\n",
    "X1=sm.add_constant(X1)\n",
    "X2=sm.add_constant(X2)\n",
    "display(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>pvalue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>const</th>\n",
       "      <td>2.402979</td>\n",
       "      <td>8.798591e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACT</th>\n",
       "      <td>0.027064</td>\n",
       "      <td>1.389927e-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         params        pvalue\n",
       "const  2.402979  8.798591e-16\n",
       "ACT    0.027064  1.389927e-02"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>pvalue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>const</th>\n",
       "      <td>1.286328</td>\n",
       "      <td>0.000238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACT</th>\n",
       "      <td>0.009426</td>\n",
       "      <td>0.383297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hsGPA</th>\n",
       "      <td>0.453456</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         params    pvalue\n",
       "const  1.286328  0.000238\n",
       "ACT    0.009426  0.383297\n",
       "hsGPA  0.453456  0.000005"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 拟合两个模型\n",
    "gpa_lm1=sm.OLS(y,X1).fit()\n",
    "gpa_lm2=sm.OLS(y,X2).fit()\n",
    "\n",
    "# 输出两个模型的系数与对应p值\n",
    "p1=pd.DataFrame(gpa_lm1.pvalues,columns=['pvalue'])\n",
    "c1=pd.DataFrame(gpa_lm1.params,columns=['params'])\n",
    "p2=pd.DataFrame(gpa_lm2.pvalues,columns=['pvalue'])\n",
    "c2=pd.DataFrame(gpa_lm2.params,columns=['params'])\n",
    "display(c1.join(p1,how='right'))\n",
    "display(c2.join(p2,how='right'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们发现，变量ACT在两个回归模型中的系数并不一致，且其在单独回归时变量显著，但增添了变量hsGPA后变得不显著。这说明多个变量共同回归绝不等同于多个变量各自进行单变量回归，且在后面的课程中我们会知道将多个重要变量都纳入回归模型的重要性。总之，大家在此只需要知道：**多元线性回归非常重要，后续的学习也将围绕多元线性回归展开！**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 模型系数的估计方法——OLS估计及其性质"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在第一章，我们了解了回归的本质——条件均值建模、介绍了最经典的回归模型——（多元）线性回归模型的形式、参数解释与一些注意事项。那么接下来有一个非常自然而然的问题摆在我们面前——你这个线性回归模型里的参数是使用什么方法计算出来的呢？按照你这种方法计算出来的参数是否可靠呢？它们又具备哪些统计性质呢？那么这一章，我们将学习线性回归中最常用、最经典的系数估计方法——普通最小二乘估计法(Ordinary Least Squares, OLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 OLS估计的思想与原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 OLS估计的思想"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们用一个关于储蓄与收入间关系的例子解释ols估计的思想。将储蓄savings视作因变量$y$，将收入income视作自变量$x$，由于只有一个自变量，因此可用简单线性回归模型假设两者关系为$y=\\beta_{0}+\\beta_{1} x+u$，即一条带有趋势与截距的直线。那么，这条直线应该“长成”怎样才算是一条“好的直线”呢？直观上看，最佳的拟合直线应该尽可能的贴合样本点，如下图所示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src='./images/ols.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "直线尽可能贴合样本点，意味着在所有直线当中，我们要选出一条离所有样本点距离的总和最小的直线。那么，这个距离该如何衡量？我们将模型回归参数分别记为$\\hat{\\beta}_{0}$，$\\hat{\\beta}_{1}$，并定义$\\hat{y}_{i}=\\hat{\\beta}_{0}+\\hat{\\beta}_{1} x_{i}$为样本在自变量为$x_i$下的拟合值，记样本实际观测值$y_i$与拟合值$\\hat{y}_{i}$之差为拟合残差$\n",
    "\\hat{u}_{i}=y_{i}-\\hat{y}_{i}$。\n",
    "\n",
    "不同的距离定义方法是不同估计法的一大区别，OLS对距离的定义是：残差的平方${\\hat{u}_i}^2$。因此OLS估计的思想是：**OLS估计求得的系数$\\hat{\\beta}_{0}$、$\\hat{\\beta}_{1}$，将使直线与所有样本的拟合残差的平方和最小**，即\n",
    "$$\n",
    "\\left(\\hat{\\beta}_{0}, \\hat{\\beta}_{1}\\right)=\\operatorname{argmin} \\sum_{i=1}^{n}\\left(y_{i}-\\hat{\\beta}_{0}-\\hat{\\beta}_{1} x_{i}\\right)^{2}\n",
    "$$\n",
    "对于多元线性回归，OLS估计的思想也完全相同，只不过多元线性回归的模型不是一条直线，而是一个多维的超平面。对于多元线性回归的OLS估计目标函数，有\n",
    "$$\n",
    "\\left( \\hat{\\beta}_0,\\cdots ,\\hat{\\beta}_k \\right) =\\mathrm{arg}\\min \\sum_{i=1}^n{\\left( y_i-\\hat{\\beta}_0-\\hat{\\beta}_1x_{1i}-\\hat{\\beta}_kx_{ki} \\right) ^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 OLS估计的求解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**· 从优化角度看OLS求解**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们在上面知晓了需要求解的函数后，接下来就要开始进行求解了。\n",
    "\n",
    "记目标函数为\n",
    "$$\n",
    "Q\\left(\\hat{\\beta}_{0}, \\hat{\\beta}_{1}, \\cdots, \\hat{\\beta}_{k}\\right)=\\sum_{i=1}^{n}\\left(y_{i}-\\hat{\\beta}_{0}-\\hat{\\beta}_{1} x_{i 1}-\\cdots-\\hat{\\beta}_{k} x_{i k}\\right)^{2}\n",
    "$$\n",
    "这是一个以$(\\hat{\\beta}_{0}, \\hat{\\beta}_{1}, \\cdots, \\hat{\\beta}_{k})$作为未知变量的多元函数，我们要求得最小值点，可以令各元偏导数等于0，构建一个$k+1$维的方程组求解：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\sum_{i=1}^{n}\\left(y_{i}-\\hat{\\beta}_{0}-\\hat{\\beta}_{1} x_{i 1}-\\cdots-\\hat{\\beta}_{k} x_{i k}\\right)=0 \\\\\n",
    "&\\sum_{i=1}^{n}\\left(y_{i}-\\hat{\\beta}_{0}-\\hat{\\beta}_{1} x_{i 1}-\\cdots-\\hat{\\beta}_{k} x_{i k}\\right) x_{i 1}=0 \\\\\n",
    "&\\sum_{i=1}^{n}\\left(y_{i}-\\hat{\\beta}_{0}-\\hat{\\beta}_{1} x_{i 1}-\\cdots-\\hat{\\beta}_{k} x_{i k}\\right) x_{i 2}=0 \\\\\n",
    "&\\cdots \\quad \\cdots \\\\\n",
    "&\\sum_{i=1}^{n}\\left(y_{i}-\\hat{\\beta}_{0}-\\hat{\\beta}_{1} x_{i 1}-\\cdots-\\hat{\\beta}_{k} x_{i k}\\right) x_{i k}=0\n",
    "\\end{aligned}\n",
    "$$\n",
    "以上方程组中，每个方程有$k+1$个自变量，且有$k+1$个方程，根据线性代数的知识，我们可以求得$(\\hat{\\beta}_{0}, \\hat{\\beta}_{1}, \\cdots, \\hat{\\beta}_{k})$的唯一解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**· OLS求解的矩阵表示**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述思路证明了OLS估计求解系数的可行性，但是我们还不知道系数估计的具体形式，接下来我们就利用矩阵的形式给出OLS求解的结果。求解过程无需掌握，但大家需要大致了解每个数学符号的含义与指代，在后续的理论介绍时，它们会被反复提及。\n",
    "\n",
    "由于我们有$n$个样本，因此根据模型有以下$n$个等式成立\n",
    "$$\n",
    "y_{i}=\\beta_{0}+\\beta_{1} x_{i 1}+\\cdots+\\beta_{k} x_{i k}+u_{i}, \\quad i=1, \\cdots, n\n",
    "$$\n",
    "将它们联立称方程组，并表示成矩阵形式\n",
    "$$\n",
    "\\boldsymbol{y}=\\boldsymbol{X\\beta }+\\boldsymbol{u}\n",
    "$$\n",
    "这里，$\\boldsymbol{y}=\\left( y_1,y_2,\\cdots ,y_n \\right) ^{'},\\quad \\boldsymbol{\\beta} =\\left( \\beta _0,\\beta _1,\\cdots ,\\beta _k \\right) ^{'},\\quad \\boldsymbol{u}=\\left( u_1,u_2,\\cdots ,u_n \\right) ^{'}$。\n",
    "<br>\n",
    "并记：$x_{i}^{\\prime}=\\left(1, x_{i 1}, x_{i 2}, \\cdots, x_{i k}\\right), \\boldsymbol{X}=\\left(x_{1}^{\\prime}, x_{2}^{\\prime}, \\cdots, x_{n}^{\\prime}\\right)^{\\prime}$，值得注意的是，$\\boldsymbol{X}$是一个$n\\times \\left( k+1 \\right) $维的矩阵，n为样本个数，k为自变量个数，它也被称为设计阵。\n",
    "\n",
    "以上是真实模型的矩阵表示形式，对于我们实际拟合的模型及其残差，其矩阵形式则为\n",
    "$$\n",
    "\\boldsymbol{\\hat{y}}=\\boldsymbol{X\\hat{\\beta}},\\quad \\boldsymbol{\\hat{u}}=\\boldsymbol{y}-\\boldsymbol{\\hat{y}}\n",
    "$$\n",
    "根据令残差平方和偏导数为0的思想，有\n",
    "$$\n",
    "Q(\\hat{\\beta})=\\sum_{i=1}^{n} \\hat{u}_{i}^{2}=\\hat{u}^{\\prime} \\hat{u}=(y-X \\hat{\\beta})^{\\prime}(y-X \\hat{\\beta})=y^{\\prime} y-2 \\hat{\\beta}^{\\prime} X^{\\prime} y+\\hat{\\beta}^{\\prime} X^{\\prime} X \\hat{\\beta}\n",
    "$$\n",
    "运用向量求导的知识得\n",
    "$$\n",
    "X^{\\prime} X \\hat{\\beta}=X^{\\prime} y \\Rightarrow \\hat{\\beta}=\\left(X^{\\prime} X\\right)^{-1} X^{\\prime} y\n",
    "$$\n",
    "至此，我们就得到了各系数估计向量$\\hat{\\beta}$的矩阵表达式了。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "手动计算的系数向量为：\n",
      "[1.28632777 0.00942601 0.45345589]\n",
      "-----------------------------------\n",
      "软件计算的系数为：\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>const</th>\n",
       "      <td>1.286328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACT</th>\n",
       "      <td>0.009426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hsGPA</th>\n",
       "      <td>0.453456</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         params\n",
       "const  1.286328\n",
       "ACT    0.009426\n",
       "hsGPA  0.453456"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 以gpa_lm2为例\n",
    "## 手动计算系数的估计向量\n",
    "X2_T=X2.values.T\n",
    "X_inv=np.linalg.inv(np.dot(X2_T,X2)) # 求矩阵乘积的逆矩阵\n",
    "Xy=np.dot(X2_T,y.values)\n",
    "beta_hat=np.dot(X_inv,Xy)\n",
    "print('手动计算的系数向量为：')\n",
    "print(beta_hat)\n",
    "\n",
    "## 软件计算的系数向量\n",
    "print('-----------------------------------')\n",
    "print('软件计算的系数为：')\n",
    "display(c2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 拟合优度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于多元线性模型\n",
    "$$\n",
    "y=\\beta_{0}+\\beta_{1} x_{1}+\\cdots+\\beta_{k} x_{k}+u\n",
    "$$\n",
    "我们使用OLS得到了一个拟合模型\n",
    "$$\n",
    "\\hat{y}=\\hat{\\beta}_0+\\hat{\\beta}_1x_1+\\cdots +\\hat{\\beta}_kx_k\n",
    "$$\n",
    "一个很自然的问题是：这个模型对数据的拟合效果如何？这个问题可以进一步引申为：模型中的自变量$x_i$在多大程度上解释了$y$的变异？（$y$的趋势变化可以理解为是一种带有规律性的变异）\n",
    "\n",
    "在探讨这个问题前，我们先引入几个简单而又重要的概念。\n",
    "\n",
    "· TSS(Total sum of squares)，总平方和\n",
    "$$\n",
    "T S S=\\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)^{2}\n",
    "$$\n",
    "\n",
    "· ESS(Explained sum of squares)，解释平方和\n",
    "$$\n",
    "E S S=\\sum_{i=1}^{n}\\left(\\hat{y}_{i}-\\bar{y}\\right)^{2}\n",
    "$$\n",
    "\n",
    "· RSS(Resiual sum of squares)，残差平方和\n",
    "$$\n",
    "R S S=\\sum_{i=1}^{n}\\left(y_{i}-\\hat{y}_{i}\\right)^{2}\n",
    "$$\n",
    "直观上RSS是一个可以度量拟合优度的量，因为残差平方和越小，意味着预测值与真实值之间的差距越小。但是RSS的大小没有一个标准，它将随着样本量的增大而增大，因此单纯的RSS不是一个合格的衡量拟合优度的量。\n",
    "\n",
    "这个时候我们可以从另一个角度去理解回归建模的意义。我们之所以想构建模型，是因为想找到**造成$y$值变化**的因素，模型解释的变异占总变异的比例越多，这个模型的解释力度就越大，模型的拟合优度也就越好。我们举一个简单的例子：某天，一个村子的菜包子涨了1块钱，大家都想知道究竟是什么原因导致这1块钱的涨幅。小红和小明综合了当天所有发生变化的外因素（其实就是自变量啦~），分别构建了两个模型将这些外因素的变化和菜包子涨价的1块钱联系在一起。在小红的模型预测下，这些外因素变化会使菜包子涨价0.99块钱，而小明的模型则只预测到了0.1块钱的涨价。我们认为，小红的模型解释1块钱涨价中的0.99块，而小明只解释了0.1块，因此小红的模型更优。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**· 回归拟合优度——R方**\n",
    "\n",
    "理解了用“解释变异的比例”衡量回归模型拟合优度的思想，构造拟合优度就有思路了。回归分析中最常用的拟合优度是R方，定义为\n",
    "$$\n",
    "R^{2}=\\frac{E S S}{T S S}\n",
    "$$\n",
    "其中，TSS度量了因变量$y$的总样本变异，而ESS度量了模型拟合值$\\hat{y}$的总变异，也就是解释了的变异。事实上三种平方和存在关系$TSS=RSS+ESS$（大家可以尝试自己推导），这说明：总变异可以被拆分为解释了变异和未被解释的变异，残差平方和度量了“剩余信息”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "手动计算的R方为：0.17642151056579924\n",
      "-------------------------------------------------\n",
      "软件计算的R方为：0.17642159463450557\n"
     ]
    }
   ],
   "source": [
    "# 动手计算模型gpa_lm2的R方\n",
    "TSS_gpa=np.sum(np.power(gpa1.colGPA-np.mean(gpa1.colGPA),2))\n",
    "RSS_gpa=np.sum(np.power(gpa_lm2.resid,2))\n",
    "gpa_lm2_R2=1-RSS_gpa/TSS_gpa\n",
    "print('手动计算的R方为：{}'.format(gpa_lm2_R2))\n",
    "print('-------------------------------------------------')\n",
    "# 直接输出模型gpa_lm2的R方\n",
    "gpa_lm2_R2=gpa_lm2.rsquared\n",
    "print('软件计算的R方为：{}'.format(gpa_lm2_R2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "两者结果十分接近，之所以不完全相同可能是numpy计算与statsmodels计算存在小差异。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4 OLS估计的代数性质"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用OLS估计对线性回归模型进行参数估计，估计出来的模型将有许多重要的特性与性质。其中有的性质是OLS估计自身求解过程所带来的，我们称之为代数性质，这部分性质是天然成立的；而有的性质只有在某些特定的模型假设下才能成立，一旦实际数据违反了假设，这些性质将不再成立。\n",
    "\n",
    "在这一小节，我们将简单学习OLS估计的代数性质。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**· 代数性质**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLS估计的代数性质来自于其本身求解过程中的方程组。我们观察一下上面的方程组，可以很快地总结出以下两条公式\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\t&\\sum_{i=1}^n{\\left( y_i-\\hat{\\beta}_0-\\hat{\\beta}_1x_{i1}-\\cdots -\\hat{\\beta}_kx_{ik} \\right)}=\\sum_{i=1}^n{\\hat{u}_i}=0\\\\\n",
    "\t&\\sum_{i=1}^n{\\left( y_i-\\hat{\\beta}_0-\\hat{\\beta}_1x_{i1}-\\cdots -\\hat{\\beta}_kx_{ij} \\right)}x_{ij}=\\sum_{i=1}^n{\\hat{u}_i}x_{ij}=0, j=1,\\cdots ,k\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "第一条公式意味着：OLS估计预测残差之和为0；此外，这可以推出预测残差的均值也为0，即$\\bar{\\hat{u}}=0$。我们以之前的gpa1回归建模为例，看看模型在python中实际计算出来的残差之和是否为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "简单回归模型的残差和（保留四位小数点）：-0.0000\n",
      "多元回归模型的残差和（保留四位小数点）：-0.0000\n"
     ]
    }
   ],
   "source": [
    "print('简单回归模型的残差和（保留四位小数点）：{:.4f}'.format(sum(gpa_lm1.resid)))\n",
    "print('多元回归模型的残差和（保留四位小数点）：{:.4f}'.format(sum(gpa_lm2.resid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二条公式可以进行以下恒等推导：\n",
    "$$\n",
    "\\sum_{i=1}^n{x_{ik}}\\hat{u}_i=\\sum_{i=1}^n{x_{ik}}\\left( \\hat{u}_i-\\bar{\\hat{u}} \\right) =\\sum_{i=1}^n{\\left( x_{ik}-\\bar{x} \\right)}\\left( \\hat{u}_i-\\bar{\\hat{u}} \\right) =Cov\\left( x_k,\\hat{u} \\right) =0, j=1,\\cdots ,k\n",
    "$$\n",
    "这是OLS估计最重要的代数性质，它意味着OLS估计的残差与参与回归的自变量不相关。这预示着：如果我们消除因变量$y$与某些自变量$x_j$之间的线性相关性，可以先进行线性回归然后取残差！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "回归模型中残差与自变量hsGPA的样本协方差为（保留四位小数点）：-0.0000\n"
     ]
    }
   ],
   "source": [
    "# 定义计算残差的计算函数\n",
    "from pylab import *\n",
    "def de_mean(x):\n",
    "    xmean = np.mean(x)\n",
    "    return [xi - xmean for xi in x]\n",
    "\n",
    "# 定义计算样本协方差的计算函数\n",
    "def covariance(x, y):\n",
    "    n = len(x)\n",
    "    return dot(de_mean(x), de_mean(y)) / (n-1)\n",
    "\n",
    "print('回归模型中残差与自变量hsGPA的样本协方差为（保留四位小数点）：{:.4f}'.format(covariance(gpa_lm2.resid,gpa1.hsGPA)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 经典线性模型假设下OLS估计的性质 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 经典线性模型假设-CLM假设"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLM假设总共有六条，在六条假设下OLS估计具有非常优良的性质，接下来让我们看看这六条假设是什么，每条意味着什么。\n",
    "\n",
    "**· MLR.1 总体模型假设** \n",
    "\n",
    "总体模型可以写为\n",
    "$$\n",
    "y=\\beta_{0}+\\beta_{1} x_{1}+\\beta_{2} x_{2}+\\cdots+\\beta_{k} x_{k}+u\n",
    "$$\n",
    "该假设假定了我们正确地判断了因变量和自变量之间的关系——既正确设定了模型形式为上述的线性形式，又正确纳入了所有自变量。\n",
    "\n",
    "**· MLR.2 随机误差条件均值零假设**\n",
    "\n",
    "随机误差$u$满足\n",
    "$$\n",
    "E\\left(u \\mid x_{1}, \\cdots, x_{k}\\right)=0\n",
    "$$\n",
    "我们在前面的小节提到过这一假设的推论，它意味着所有非自变量的其他因素都与自变量线性无关。\n",
    "\n",
    "**· MLR.3 随机抽样假设**\n",
    "\n",
    "$n$个来自上述总体的样本均为随机抽样样本，彼此之间相互独立\n",
    "\n",
    "**· MLR.4 非完全共线性假设**\n",
    "\n",
    "这些样本的所有自变量间不能存在有完全共线性，即不能存在某一自变量可由其余自变量进行线性表示的情况。数学语言为：不存在不全为零的$a_{0}, a_{1}, \\cdots, a_{k}$使得\n",
    "$$\n",
    "a_{0}+a_{1} x_{i 1}+a_{2} x_{i 2}+\\cdots+a_{k} x_{i k}=0, \\forall i=1, \\cdots, n\n",
    "$$\n",
    "**· MLR.5 同方差假设**\n",
    "\n",
    "随机误差$u$的条件方差恒为一个常数，即\n",
    "$$\n",
    "\\operatorname{Var}\\left(u \\mid x_{1}, \\cdots, x_{k}\\right)=\\sigma^{2}\n",
    "$$\n",
    "根据条件方差的性质，上述等式可等价为(大家可以想想为什么会这样呢？)\n",
    "$$\n",
    "\\operatorname{Var}\\left(y \\mid x_{1}, \\cdots, x_{k}\\right)=\\sigma^{2}\n",
    "$$\n",
    "同方差假设看起来有一点点抽象，但其实它非常好理解也非常直观——数据的波动程度不受自变量影响，不论$x_i$如何变化，数据与样本条件均值的偏离程度都是恒定的。我们看看以下两张对比图，直观地感受同方差与异方差的区别。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src='./images/同方差.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随着$X$的增大，左图样本数据间的距离越来越大，这是方差越来越大的体现；而右图样本数据间的距离则相对稳定，这说明它们的方差相对稳定。\n",
    "\n",
    "**· MLR.6 正态性假设**\n",
    "\n",
    "该假设假定随机误差$u$在任何自变量$x$已知的条件下服从正态分布\n",
    "$$\n",
    "u \\mid x \\sim N\\left(0, \\sigma^{2}\\right)\n",
    "$$\n",
    "这一假设实际上是MLR.2与MLR.5假设的升级版，即在随机误差$u$的零条件期望与恒定条件方差的基础上，增加了一个服从条件正态分布的假设。\n",
    "\n",
    "以上六个假设是一种非常严格、理想化的假设，只有在这些假设成立的基础上我们才能对OLS估计在线性回归模型上的性质作进一步的研究。当然，实际的数据并不一定都能满足这些假设，有关样本数据是否可以满足这些假设的识别检验、不满足假设的后果以及改进方案，我们将在以后的章节学习。接下来，对于一个样本数据，请大家默认其满足CLM假设，我们将见识到OLS系数估计法在CLM假设下的优越性。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 OLS估计的性质-最优的线性无偏估计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们之所以对样本数据进行建模并估计模型的系数，是因为我们认为存在一个潜在的、**正确的**模型（函数）可以描述这些数据的特征。在回归任务中，这个我们假定正确的函数被称为**总体回归函数**\n",
    "$$\n",
    "y=\\beta_{0}+\\beta_{1} x_{1}+\\cdots+\\beta_{k} x_{k}+u\n",
    "$$\n",
    "而我们使用各种模型估计方法得出的对这个总体回归函数的函数则被称为**样本回归函数**\n",
    "$$\n",
    "\\hat{y}=\\hat{\\beta}_0+\\hat{\\beta}_1x_1+\\cdots +\\hat{\\beta}_kx_k\n",
    "$$\n",
    "对于一个样本回归函数而言，怎样子的函数才算是好函数呢？答案当然是，各估计系数$\\hat{\\beta}$都尽可能接近真实系数$\\beta$；并且使用同一总体的不同取样样本进行估计时，估计出来的系数越稳定越好。而OLS估计在这两方面的表现都非常不错。\n",
    "\n",
    "**· OLS系数估计的无偏性**\n",
    "\n",
    "**定理1.** 在CLM假设**MLR.1-MLR.4**下，$\\hat{\\beta}$是$\\beta$的无偏估计，即\n",
    "$$\n",
    "E\\left(\\hat{\\beta}_{j}\\right)=\\beta_{j}, \\forall j=0,1, \\cdots, k\n",
    "$$\n",
    "\n",
    "无偏性意味着我们使用OLS进行多次试验后，估计出来的系数均值与参数的真实值是吻合的，这是一件激励人心的事，这说明我们估计出来的系数非常接近真实系数！接下来我们看看估计系数的稳定性——方差。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**· OLS系数估计的方差**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们先看一下CLM假设下，OLS估计系数的**方差与标准误**具体为多少。\n",
    "\n",
    "**· 定理2.** 在CLM假设**MLR.1-MLR.5**（增加了同方差假设MLR.5)下，$\\hat{\\beta}$的方差-协方差矩阵表达式为\n",
    "$$\n",
    "\\operatorname{Cov}(\\hat{\\beta})=\\sigma^{2}\\left(X^{\\prime} X\\right)^{-1}\n",
    "$$\n",
    "其中，$X$为设计阵；$\\sigma^{2}$为同方差假设$\\operatorname{Var}\\left(u \\mid x_{1}, \\cdots, x_{k}\\right)=\\sigma^{2}$中的随机误差的方差。\n",
    "\n",
    "由于$\\hat{\\beta}$向量的协方差矩阵对角线就是每个系数的方差，因此有OLS估计的方差以及标准差(SD,Standard Deviation)\n",
    "$$\n",
    "\\operatorname{Var}\\left(\\hat{\\beta}_{j}\\right)=\\sigma^{2}\\left(X^{\\prime} X\\right)_{j+1, j+1}^{-1}\n",
    "$$\n",
    "$$\n",
    "\\operatorname{sd}\\left(\\hat{\\beta}_{j}\\right)=\\sigma \\sqrt{\\left(X^{\\prime} X\\right)_{j+1, j+1}^{-1}}\n",
    "$$\n",
    "我们已经知道了各估计系数$\\hat{\\beta}_{j}$的方差表达式了，但是还有最后一个问题需要解决：$\\sigma^{2}$是我们假设的一个参数，实际我们并不知道它是多少，因此我们要给出它的估计。\n",
    "\n",
    "既然$\\sigma^{2}$是随机误差的方差，那么它的估计形式总体上应该遵循样本方差的形式，即有这样的形式\n",
    "$$\n",
    "\\hat{\\sigma}^2=\\frac{1}{df}\\sum_{i=1}^n{\\left( \\hat{u} \\right) ^2}\n",
    "$$\n",
    "其中，$df$是自由度，它一般是样本个数与待估计参数个数的差。而对于随机误差的估计形式$\\hat{u}$，我们回想一下随机误差的含义：它是样本真值$y$与总体回归函数$m(x)$的偏差，其构造是不是与残差十分相似呢？因此在对随机误差的估计中，我们就用残差代替随机误差，故有\n",
    "$$\n",
    "\\hat{\\sigma}^{2}=\\frac{1}{n-k-1} \\sum_{i=1}^{n}\\left(y_{i}-\\hat{\\beta}_{0}-\\hat{\\beta}_{1} x_{i 1}-\\cdots-\\hat{\\beta}_{k} x_{i k}\\right)^{2}=\\frac{RSS}{n-k-1}\n",
    "$$\n",
    "$\\hat{\\sigma}$被称为回归标准误(standard error of regression)，我们将$\\hat{\\sigma}$带入到上述的标准差$\\operatorname{sd}\\left(\\hat{\\beta}_{j}\\right)$当中，得到的结果被称为估计系数的标准误(standard error)\n",
    "$$\n",
    "\\operatorname{se}\\left(\\hat{\\beta}_{j}\\right)=\\hat{\\sigma} \\sqrt{\\left(X^{\\prime} X\\right)_{j+1, j+1}^{-1}}\n",
    "$$\n",
    "在这里我们需要明确地指出，因为$\\sigma$的未知性，估计系数标准差$\\operatorname{sd}\\left(\\hat{\\beta}_{j}\\right)$是在实际中无法得知的，python软件也不会输出这个指标。只有估计系数标准误$\\operatorname{se}\\left(\\hat{\\beta}_{j}\\right)$才是明确的！\n",
    "\n",
    "**· 定理3.** 在CLM假设**MLR.1-MLR.5**下，$\\hat{\\sigma}^{2}$是$\\sigma^{2}$的无偏估计，即\n",
    "$$\n",
    "E\\left(\\hat{\\sigma}^{2}\\right)=\\sigma^{2}\n",
    "$$\n",
    "这个定理表明，在CLM假设下，我们上述对随机误差的方差的估计是“准确的”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "手动计算的回归标准误：0.1158148144382463\n",
      "软件计算的回归标准误：0.1158148144382463\n",
      "-------------------------------------------------\n",
      "手动计算的ACT系数标准误：0.010777187759672808\n",
      "软件计算的ACT系数标准误：0.010777187759672789\n"
     ]
    }
   ],
   "source": [
    "# 依旧以gpa_lm2模型为例\n",
    "# 手动计算标准误，并比较python直接输出的结果\n",
    "\n",
    "# 回归标准误\n",
    "## 手动计算\n",
    "df=gpa_lm2.df_resid # 计算自由度\n",
    "sigma=RSS_gpa/df\n",
    "print('手动计算的回归标准误：{}'.format(sigma))\n",
    "## 软件输出\n",
    "sigma2=gpa_lm2.scale\n",
    "print('软件计算的回归标准误：{}'.format(sigma2))\n",
    "print('-------------------------------------------------')\n",
    "\n",
    "# 变量ACT系数的标准误\n",
    "## 手动计算\n",
    "X2_T=X2.values.T\n",
    "X_inv=np.linalg.inv(np.dot(X2_T,X2)) # 求矩阵乘积的逆矩阵\n",
    "se_beta1=np.sqrt(sigma*X_inv[(1,1)])\n",
    "print('手动计算的ACT系数标准误：{}'.format(se_beta1))\n",
    "\n",
    "## 软件输出\n",
    "se_beta1=gpa_lm2.bse[1]\n",
    "print('软件计算的ACT系数标准误：{}'.format(se_beta1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们也可以直接使用接口summary来直观的展示模型拟合的各种指标结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 colGPA   R-squared:                       0.176\n",
      "Model:                            OLS   Adj. R-squared:                  0.164\n",
      "Method:                 Least Squares   F-statistic:                     14.78\n",
      "Date:                Wed, 07 Sep 2022   Prob (F-statistic):           1.53e-06\n",
      "Time:                        11:22:02   Log-Likelihood:                -46.573\n",
      "No. Observations:                 141   AIC:                             99.15\n",
      "Df Residuals:                     138   BIC:                             108.0\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          1.2863      0.341      3.774      0.000       0.612       1.960\n",
      "ACT            0.0094      0.011      0.875      0.383      -0.012       0.031\n",
      "hsGPA          0.4535      0.096      4.733      0.000       0.264       0.643\n",
      "==============================================================================\n",
      "Omnibus:                        3.056   Durbin-Watson:                   1.885\n",
      "Prob(Omnibus):                  0.217   Jarque-Bera (JB):                2.469\n",
      "Skew:                           0.199   Prob(JB):                        0.291\n",
      "Kurtosis:                       2.488   Cond. No.                         298.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "print(gpa_lm2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们在前面使用代码实现的各种指标，大部分都可以在summary汇总表格中找到。如R-squared就是R方，Df Residuals就是模型自由度；估计系数部分，第一列是系数的估计值，第二列是估计系数的标准误，第三第四列则是我们在下一章节介绍的系数显著性指标。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**· OLS系数估计的最优线性无偏性**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在MLR.1-MLR.4下，我们知道了OLS估计是无偏的；在MLR.1-MLR.5下，我们得出了OLS估计方差的表达式。那么在这里我们要告诉大家，在所有无偏估计当中，OLS估计是最优的，因为有如下定理\n",
    "\n",
    "**· Gauss-Markov定理.** 在CLM假设**MLR.1-MLR.5**下，在$\\beta$的所有线性无偏估计类当中，OLS估计的方差最小。即假设另有无偏估计$\\tilde{\\beta}_{j}$，若它可以表示为$y_i$的线性组合，则必有\n",
    "$$\n",
    "\\operatorname{Var}\\left(\\hat{\\beta}_{j}\\right)<\\operatorname{Var}\\left(\\tilde{\\beta}_{j}\\right)\n",
    "$$\n",
    "值得注意的是，OLS只是在线性无偏估计中的方差最小，如果我们不追求估计的无偏性而只追求估计的稳定性（小方差），可以采用岭估计等有偏估计。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**· OLS系数估计的抽样分布-t分布**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在MLR.1-MLR.5下，OLS估计是最优的线性无偏估计，这充分彰显了它的优越性。我们注意到，还有一个MLR.6正态性假设的没被用上，那么这个假设的作用是什么呢？——它确定了估计系数$\\hat{\\beta}_{j}$服从的分布，这为回归分析中最重要的一项功能——模型的假设检验打下了坚实基础。\n",
    "\n",
    "**· 定理4.** 在CLM假设**MLR.1-MLR.6**下，$\\hat{\\beta}_{j}$服从正态分布\n",
    "$$\n",
    "\\hat{\\beta}_{j} \\sim N\\left(\\beta_{j}, \\operatorname{Var}\\left(\\hat{\\beta}_{j}\\right)\\right)\n",
    "$$\n",
    "聪明的小伙伴们可能马上意识到，这样子不就有\n",
    "$$\n",
    "\\frac{\\hat{\\beta}_{j}-\\beta_{j}}{s d\\left(\\hat{\\beta}_{j}\\right)} \\sim N(0,1)\n",
    "$$\n",
    "我们就可以使用正态分布进行假设检验了吗？非也！因为这里有一个致命的问题：标准差$\\operatorname{sd}\\left(\\hat{\\beta}_{j}\\right)$在实际问题当中是无法求解的，我们也就无法通过构造一个含有$\\operatorname{sd}\\left(\\hat{\\beta}_{j}\\right)$的检验统计量进行假设检验了，因为它无法被计算出来。不过这难不到统计学家们，因为标准误$\\operatorname{se}\\left(\\hat{\\beta}_{j}\\right)$是可以被计算出来的，并且有\n",
    "$$\n",
    "\\frac{\\hat{\\beta}_{j}-\\beta_{j}}{\\operatorname{se}\\left(\\hat{\\beta}_{j}\\right)} \\sim t_{n-k-1}\n",
    "$$\n",
    "也就是说，统计量$\\frac{\\hat{\\beta}_{j}-\\beta_{j}}{\\operatorname{se}\\left(\\hat{\\beta}_{j}\\right)}$是服从t分布的，我们就可以使用t分布进行模型的假设检验了。注意：这些结论都建立在MLR.6正态性假设成立的基础上！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7758e92e9a61d7a3490898707f7eeb937c85e9d1e8d4e877cc6c187218f226d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
